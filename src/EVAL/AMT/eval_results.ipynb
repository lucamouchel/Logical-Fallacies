{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "analysis = 'CPO'\n",
    "df = pd.read_csv(f'/root/Logical-Fallacies/src/EVAL/AMT/annotations/SFT_vs_{analysis}.csv')\n",
    "df = df [['Input.topic', 'Input.stance', 'Input.argument1', 'Input.argument2', 'Input.sft_index', 'Answer.q1_valid']]\n",
    "gpt_4_fallacies = pd.read_json(f'/root/Logical-Fallacies/results/llama/arguments/{analysis.lower()}/f-rate.json')\n",
    "sft_gpt_4 = pd.read_json(f'/root/Logical-Fallacies/results/llama/arguments/sft/f-rate.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_topic = (df['Input.topic'].unique())\n",
    "seen_topics = {topic: [] for topic in set_topic}\n",
    "for i, entry in df.iterrows():\n",
    "    seen_topics[entry['Input.topic']].append(entry['Answer.q1_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of times humans selected CPO and it was NOT considered a fallacy:  0.667\n",
      "% of times humans selected TIE and CPO was NOT considered a fallacy:  0.643\n",
      "% of times humans selected SFT and it was NOT considered a fallacy:  0.717\n",
      "% of times human selected CPO when CPO was a fallacy and SFT was not a fallacy:  0.086\n",
      "% of times human selected SFT when CPO was not a fallacy and SFT was a fallacy:  0.059\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "is_fallacy = []\n",
    "preds = []\n",
    "\n",
    "\n",
    "results = {'SFT': 0, analysis: 0, 'tie_good': 0, 'tie_bad': 0}\n",
    "l, l2=0, 0\n",
    "K=0\n",
    "good, bad = 0,0\n",
    "A,B=0,0\n",
    "L = 0\n",
    "comparisons = []\n",
    "\n",
    "T , X = 0,0\n",
    "total = 0\n",
    "for k,v in seen_topics.items():\n",
    "    \n",
    "    sft_index = df[df['Input.topic'] == k].iloc[0]['Input.sft_index']\n",
    "    counts = Counter(v)\n",
    "    gpt_4 = gpt_4_fallacies[gpt_4_fallacies.topic==k].iloc[0]\n",
    "\n",
    "    sft = sft_gpt_4[sft_gpt_4.topic==k].iloc[0]\n",
    "    sft_f_type = sft.fallacy_type\n",
    "\n",
    "    argument = gpt_4.argument\n",
    "    f_type = gpt_4.fallacy_type\n",
    "\n",
    "   \n",
    "    if not all([count == 1 for count in counts.values()]):\n",
    "        maximum_frequency = max(counts, key=counts.get)\n",
    "        if maximum_frequency == -10:\n",
    "            results['tie_bad'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency == 10:\n",
    "            results['tie_good'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency in [1,2]:\n",
    "            maximum_frequency -= 1\n",
    "            if maximum_frequency != sft_index:\n",
    "                results[analysis] += 1\n",
    "                winner = analysis\n",
    "            else:\n",
    "                results['SFT'] += 1\n",
    "                winner = 'SFT'\n",
    "\n",
    "        if winner =='tie' :\n",
    "            if f_type == 'None':\n",
    "                T += 1\n",
    "            else :\n",
    "                X += 1\n",
    "        if (winner == analysis):\n",
    "            if f_type == 'None':\n",
    "                l += 1\n",
    "            else: \n",
    "                l2 +=1\n",
    "\n",
    "        if (winner == 'SFT'):\n",
    "            if sft_f_type == 'None':\n",
    "                A += 1\n",
    "            else :\n",
    "                B += 1\n",
    "        if winner==analysis and f_type != 'None' and sft_f_type == 'None':\n",
    "            L += 1\n",
    "        if winner == 'SFT' and f_type == 'None' and sft_f_type != 'None':\n",
    "            K += 1\n",
    "\n",
    "        if winner == 'SFT' and f_type == 'None':\n",
    "            comparisons.append((argument, f_type, sft_f_type, winner))\n",
    "\n",
    "print(f\"% of times humans selected {analysis} and it was NOT considered a fallacy: \", round(l/(results[analysis]), 3))\n",
    "print(f\"% of times humans selected TIE and {analysis} was NOT considered a fallacy: \", round(T/(results['tie_good']+results['tie_bad']), 3))\n",
    "print(f\"% of times humans selected SFT and it was NOT considered a fallacy: \", round( A/results['SFT'], 3))\n",
    "print(f\"% of times human selected {analysis} when {analysis} was a fallacy and SFT was not a fallacy: \", round(L/len(seen_topics), 3))\n",
    "print(f\"% of times human selected SFT when {analysis} was not a fallacy and SFT was a fallacy: \", round(K/len(seen_topics),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666,\n",
       " 0.7166666666666667,\n",
       " 0.0855614973262032,\n",
       " 0.058823529411764705)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l/results[analysis], A/results['SFT'], L/len(seen_topics), K/len(seen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DPO. 79.5% of the time they select DPO as a winner, it is not a fallacy. So only 20% of the time they select DPO as a fallacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should detect when the humans have chosen a sample, if they select a fallacy statement. So basically, when they pick DPO for example, does gpt4 classify it as a fallacy? if yes, then it's bad, otherwise it's good and DPO truly is better. \n",
    "When they select SFT, do they select a fallacy statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26737967914438504\n",
      "0.22994652406417113\n"
     ]
    }
   ],
   "source": [
    "print(l/len(seen_topics))\n",
    "print(A/len(seen_topics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should penalize if it chose a fallacy statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 60, 'CPO': 75, 'tie_good': 7, 'tie_bad': 7}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'SFT': results['SFT'], analysis: results[analysis], 'tie': results['tie_good'] + results['tie_bad']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT: 60 (40.27%)\n",
      "CPO: 75 (50.34%)\n",
      "tie: 14 (9.40%)\n"
     ]
    }
   ],
   "source": [
    "total = sum(d.values())\n",
    "for k,v in d.items():\n",
    "    print(f'{k}: {v} ({v/total*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
