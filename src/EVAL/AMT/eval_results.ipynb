{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "analysis = 'CPO_CUSTOM'\n",
    "df = pd.read_csv(f'/root/Logical-Fallacies/src/EVAL/AMT/annotations/SFT_vs_{analysis}.csv')\n",
    "df = df [['Input.topic', 'Input.stance', 'Input.argument1', 'Input.argument2', 'Input.sft_index', 'Answer.q1_valid']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_topic = (df['Input.topic'].unique())\n",
    "seen_topics = {topic: [] for topic in set_topic}\n",
    "for i, entry in df.iterrows():\n",
    "    seen_topics[entry['Input.topic']].append(entry['Answer.q1_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "is_fallacy = []\n",
    "preds = []\n",
    "\n",
    "\n",
    "results = {'SFT': 0, analysis: 0, 'tie_good': 0, 'tie_bad': 0}\n",
    "l, l2=0, 0\n",
    "K=0\n",
    "good, bad = 0,0\n",
    "A,B=0,0\n",
    "L = 0\n",
    "comparisons = []\n",
    "\n",
    "T , X = 0,0\n",
    "total = 0\n",
    "for k,v in seen_topics.items():\n",
    "    \n",
    "    sft_index = df[df['Input.topic'] == k].iloc[0]['Input.sft_index']\n",
    "    counts = Counter(v)\n",
    "\n",
    "   \n",
    "    if not all([count == 1 for count in counts.values()]):\n",
    "        maximum_frequency = max(counts, key=counts.get)\n",
    "        if maximum_frequency == -10:\n",
    "            results['tie_bad'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency == 10:\n",
    "            results['tie_good'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency in [1,2]:\n",
    "            maximum_frequency -= 1\n",
    "            if maximum_frequency != sft_index:\n",
    "                results[analysis] += 1\n",
    "                winner = analysis\n",
    "            else:\n",
    "                results['SFT'] += 1\n",
    "                winner = 'SFT'\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 34, 'CPO_CUSTOM': 67, 'tie_good': 36, 'tie_bad': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 34, 'CPO_CUSTOM': 67, 'tie_good': 36, 'tie_bad': 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666,\n",
       " 0.7166666666666667,\n",
       " 0.0855614973262032,\n",
       " 0.058823529411764705)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l/results[analysis], A/results['SFT'], L/len(seen_topics), K/len(seen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DPO. 79.5% of the time they select DPO as a winner, it is not a fallacy. So only 20% of the time they select DPO as a fallacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should detect when the humans have chosen a sample, if they select a fallacy statement. So basically, when they pick DPO for example, does gpt4 classify it as a fallacy? if yes, then it's bad, otherwise it's good and DPO truly is better. \n",
    "When they select SFT, do they select a fallacy statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26737967914438504\n",
      "0.22994652406417113\n"
     ]
    }
   ],
   "source": [
    "print(l/len(seen_topics))\n",
    "print(A/len(seen_topics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should penalize if it chose a fallacy statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 60, 'CPO': 75, 'tie_good': 7, 'tie_bad': 7}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'SFT': results['SFT'], analysis: results[analysis], 'tie': results['tie_good'] + results['tie_bad']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT: 60 (40.27%)\n",
      "CPO: 75 (50.34%)\n",
      "tie: 14 (9.40%)\n"
     ]
    }
   ],
   "source": [
    "total = sum(d.values())\n",
    "for k,v in d.items():\n",
    "    print(f'{k}: {v} ({v/total*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.03125\n",
       "1      0.04375\n",
       "2      0.06250\n",
       "3      0.14375\n",
       "4      0.16250\n",
       "        ...   \n",
       "138    1.00000\n",
       "139    1.00000\n",
       "140    1.00000\n",
       "141    1.00000\n",
       "142    1.00000\n",
       "Name: misunderstood-firebrand-238 - train/rewards/accuracies, Length: 143, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/root/Logical-Fallacies/src/EVAL/AMT/rewardsaccuracies.csv')['misunderstood-firebrand-238 - train/rewards/accuracies']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
