{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "analysis = 'FIPO'\n",
    "df = pd.read_csv(f'/Users/lucamouchel/epfl/logical-fallacies-repo/Logical-Fallacies/src/EVAL/AMT/annotations/SFT_vs_{analysis}.csv')\n",
    "df = df [['Input.topic', 'Input.stance', 'Input.argument1', 'Input.argument2', 'Input.sft_index', 'Answer.q1_valid']]\n",
    "gpt_4_fallacies = pd.read_json(f'/Users/lucamouchel/epfl/logical-fallacies-repo/Logical-Fallacies/results/llama_bis/cpo_results/f-rate.json')\n",
    "sft_gpt_4 = pd.read_json(f'/Users/lucamouchel/epfl/logical-fallacies-repo/Logical-Fallacies/results/llama/arguments/sft/f-rate.json')\n",
    "set_topic = (df['Input.topic'].unique())\n",
    "seen_topics = {topic: [] for topic in set_topic}\n",
    "for i, entry in df.iterrows():\n",
    "    seen_topics[entry['Input.topic']].append(entry['Answer.q1_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of times humans selected FIPO and it was NOT considered a fallacy:  0.821\n",
      "% of times humans selected TIE and FIPO was NOT considered a fallacy:  0.844\n",
      "% of times humans selected SFT and it was NOT considered a fallacy:  0.765\n",
      "% of times human selected FIPO when FIPO was a fallacy  0.294\n",
      "% of times human selected SFT when FIPO was not a fallacy and SFT was a fallacy:  0.037\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "is_fallacy = []\n",
    "preds = []\n",
    "\n",
    "\n",
    "results = {'SFT': 0, analysis: 0, 'tie_good': 0, 'tie_bad': 0}\n",
    "l, l2=0, 0\n",
    "K=0\n",
    "good, bad = 0,0\n",
    "A,B=0,0\n",
    "L = 0\n",
    "comparisons = []\n",
    "\n",
    "T , X = 0,0\n",
    "total = 0\n",
    "for k,v in seen_topics.items():\n",
    "    \n",
    "    sft_index = df[df['Input.topic'] == k].iloc[0]['Input.sft_index']\n",
    "    counts = Counter(v)\n",
    "    gpt_4 = gpt_4_fallacies[gpt_4_fallacies.topic==k].iloc[0]\n",
    "\n",
    "    sft = sft_gpt_4[sft_gpt_4.topic==k].iloc[0]\n",
    "    sft_f_type = sft.fallacy_type\n",
    "\n",
    "    argument = gpt_4.argument\n",
    "    f_type = gpt_4.fallacy_type\n",
    "\n",
    "   \n",
    "    if not all([count == 1 for count in counts.values()]):\n",
    "        maximum_frequency = max(counts, key=counts.get)\n",
    "        if maximum_frequency == -10:\n",
    "            results['tie_bad'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency == 10:\n",
    "            results['tie_good'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency in [1,2]:\n",
    "            maximum_frequency -= 1\n",
    "            if maximum_frequency != sft_index:\n",
    "                results[analysis] += 1\n",
    "                winner = analysis\n",
    "            else:\n",
    "                results['SFT'] += 1\n",
    "                winner = 'SFT'\n",
    "\n",
    "        if winner =='tie' :\n",
    "            if f_type == 'None':\n",
    "                T += 1\n",
    "            else :\n",
    "                X += 1\n",
    "        if (winner == analysis):\n",
    "            if f_type == 'None':\n",
    "                l += 1\n",
    "            else: \n",
    "                l2 +=1\n",
    "\n",
    "        if (winner == 'SFT'):\n",
    "            if sft_f_type == 'None':\n",
    "                A += 1\n",
    "            else :\n",
    "                B += 1\n",
    "        if winner==analysis and f_type == 'None':\n",
    "            L += 1\n",
    "        if winner == 'SFT' and f_type == 'None' and sft_f_type != 'None':\n",
    "            K += 1\n",
    "\n",
    "        if winner == 'SFT' and f_type == 'None':\n",
    "            comparisons.append((argument, f_type, sft_f_type, winner))\n",
    "\n",
    "print(f\"% of times humans selected {analysis} and it was NOT considered a fallacy: \", round(l/(results[analysis]), 3))\n",
    "print(f\"% of times humans selected TIE and {analysis} was NOT considered a fallacy: \", round(T/(results['tie_good']+results['tie_bad']), 3))\n",
    "print(f\"% of times humans selected SFT and it was NOT considered a fallacy: \", round( A/results['SFT'], 3))\n",
    "print(f\"% of times human selected {analysis} when {analysis} was a fallacy \", round(L/len(seen_topics), 3))\n",
    "print(f\"% of times human selected SFT when {analysis} was not a fallacy and SFT was a fallacy: \", round(K/len(seen_topics),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 34, 'FIPO': 67, 'tie_good': 36, 'tie_bad': 9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "is_fallacy = []\n",
    "preds = []\n",
    "\n",
    "\n",
    "results = {'SFT': 0, analysis: 0, 'tie_good': 0, 'tie_bad': 0}\n",
    "\n",
    "winners = []\n",
    "miss =0 \n",
    "import json\n",
    "with open('/root/Logical-Fallacies/src/EVAL/AMT/winners.json', 'r') as f:\n",
    "    gpt_4 = json.load(f)\n",
    "\n",
    "\n",
    "i = 0\n",
    "exact = 0\n",
    "new_gpt_4 = []\n",
    "sft_stuff = []\n",
    "for  GPT_4, (k,v) in (zip(gpt_4, seen_topics.items())):\n",
    "\n",
    "    sft_index = df[df['Input.topic'] == k].iloc[0]['Input.sft_index']\n",
    "    counts = Counter(v)\n",
    "\n",
    "    if not all([count == 1 for count in counts.values()]):\n",
    "        new_gpt_4.append(GPT_4)\n",
    "        \n",
    "        maximum_frequency = max(counts, key=counts.get)\n",
    "        if maximum_frequency == -10:\n",
    "            results['tie_bad'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency == 10:\n",
    "            results['tie_good'] += 1\n",
    "            winner = 'tie'\n",
    "        elif maximum_frequency in [1,2]:\n",
    "            maximum_frequency -= 1\n",
    "            if maximum_frequency != sft_index:\n",
    "                results[analysis] += 1\n",
    "                winner = 'custom'\n",
    "            else:\n",
    "                results['SFT'] += 1\n",
    "                winner = 'sft'\n",
    "        sft_stuff.append(winner)\n",
    "        if GPT_4 == 'custom' and winner == 'sft':\n",
    "            miss += 1\n",
    "        if winner == GPT_4:\n",
    "            exact += 1\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'sft': 53, 'custom': 88, 'tie': 5})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(new_gpt_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agree = 0\n",
    "for gpt, sft in zip(new_gpt_4, sft_stuff):\n",
    "    num_agree += gpt == sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'sft': 0, 'custom': 1, 'tie': 2}\n",
    "\n",
    "new_gpt_4 = [1 if x == 'sft' else 2 if x == 'custom' else 3 for x in new_gpt_4]\n",
    "sft_stuff = [1 if x == 'sft' else 2 if x == 'custom' else 3 for x in sft_stuff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11517173326591945"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = np.corrcoef(new_gpt_4, sft_stuff)[0, 1]\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 34, 'CPO_CUSTOM': 67, 'tie_good': 36, 'tie_bad': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 34, 'CPO_CUSTOM': 67, 'tie_good': 36, 'tie_bad': 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666,\n",
       " 0.7166666666666667,\n",
       " 0.0855614973262032,\n",
       " 0.058823529411764705)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l/results[analysis], A/results['SFT'], L/len(seen_topics), K/len(seen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DPO. 79.5% of the time they select DPO as a winner, it is not a fallacy. So only 20% of the time they select DPO as a fallacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should detect when the humans have chosen a sample, if they select a fallacy statement. So basically, when they pick DPO for example, does gpt4 classify it as a fallacy? if yes, then it's bad, otherwise it's good and DPO truly is better. \n",
    "When they select SFT, do they select a fallacy statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26737967914438504\n",
      "0.22994652406417113\n"
     ]
    }
   ],
   "source": [
    "print(l/len(seen_topics))\n",
    "print(A/len(seen_topics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we should penalize if it chose a fallacy statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SFT': 60, 'CPO': 75, 'tie_good': 7, 'tie_bad': 7}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'SFT': results['SFT'], analysis: results[analysis], 'tie': results['tie_good'] + results['tie_bad']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT: 60 (40.27%)\n",
      "CPO: 75 (50.34%)\n",
      "tie: 14 (9.40%)\n"
     ]
    }
   ],
   "source": [
    "total = sum(d.values())\n",
    "for k,v in d.items():\n",
    "    print(f'{k}: {v} ({v/total*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.03125\n",
       "1      0.04375\n",
       "2      0.06250\n",
       "3      0.14375\n",
       "4      0.16250\n",
       "        ...   \n",
       "138    1.00000\n",
       "139    1.00000\n",
       "140    1.00000\n",
       "141    1.00000\n",
       "142    1.00000\n",
       "Name: misunderstood-firebrand-238 - train/rewards/accuracies, Length: 143, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/root/Logical-Fallacies/src/EVAL/AMT/rewardsaccuracies.csv')['misunderstood-firebrand-238 - train/rewards/accuracies']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
