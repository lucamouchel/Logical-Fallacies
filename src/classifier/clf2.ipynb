{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/dpo_with_fallacies_test.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/train.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "new_test_data = []\n",
    "train_topics = [entry['prompt'].split('topic: ')[-1].strip() for entry in train]\n",
    "found = 0\n",
    "for entry in test:\n",
    "    topic = entry['prompt'].split('topic: ')[-1].strip()\n",
    "    if topic in train_topics:\n",
    "        found+=1\n",
    "        continue \n",
    "    new_test_data.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4,10, 11,13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, model_name, num_labels=14) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.to('cuda')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def encode(self, data, padding=True, truncation=True, return_tensors='pt'):\n",
    "        texts = data['text'].tolist()\n",
    "        return self.tokenizer(texts, padding=padding, truncation=truncation, return_tensors=return_tensors)\n",
    "    \n",
    "    def data_tensor(self, data):\n",
    "        encoded = self.encode(data)\n",
    "        return torch.tensor(encoded.input_ids), torch.tensor(encoded.attention_mask), torch.tensor(data['label'])\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def convert_data(data, split='train'):\n",
    "    num_chosen = 0\n",
    "    dataset = []\n",
    "    chosen_samples = set()\n",
    "    for entry in data:\n",
    "        instruction = entry['prompt']\n",
    "        chosen = entry['chosen']\n",
    "        rejected = entry['rejected']\n",
    "        if chosen not in chosen_samples:\n",
    "            num_chosen += 1\n",
    "            chosen_samples.add(chosen)\n",
    "            if num_chosen <= 500 and split=='train':\n",
    "                dataset.append({'text': instruction  + ' ' + chosen, 'label': 0})\n",
    "            elif split =='test' and num_chosen <= 120:\n",
    "                dataset.append({'text': chosen, 'label': 0})\n",
    "                \n",
    "        dataset.append({'text': instruction + ' ' + rejected, 'label': entry['fallacy_type']})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'howey/electra-large-mnli'\n",
    "model = Model(model_name, num_labels=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('json', data_files='/root/Logical-Fallacies/data/dpo/arguments/train.json', split='train')\n",
    "data = pd.DataFrame(data)\n",
    "def downsample_data(data, split='train'):\n",
    "    training_data = []\n",
    "    topics = []\n",
    "    num_chosen = 0\n",
    "    for i, entry in data.iterrows():\n",
    "        topic=entry.prompt.split('topic: ')[-1].strip()\n",
    "        if topic in topics:\n",
    "            continue\n",
    "        topics.append(topic)        \n",
    "        \n",
    "        chosen = entry.chosen\n",
    "        num_chosen += 1\n",
    "        rejected = entry.rejected\n",
    "        if num_chosen <= 150 and split=='train':\n",
    "            training_data.append({'text': entry.prompt + ' ' + chosen, 'label':0})\n",
    "        elif split == 'test' and num_chosen <= 25:\n",
    "            training_data.append({'text': chosen, 'label': 0})\n",
    "        training_data.append({'text': entry.prompt + ', ' + rejected, 'label': entry.fallacy_type})\n",
    "    return (pd.DataFrame(training_data))\n",
    "\n",
    "from datasets import Dataset\n",
    "import json\n",
    "data = downsample_data(data, 'train')\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/dpo_with_fallacies_test.json') as f:\n",
    "    test_dataset = json.load(f)\n",
    "test_dataset = downsample_data(pd.DataFrame(test_dataset), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train\n",
    "len(new_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_test_data = []\n",
    "for entry in new_test_data:\n",
    "    new_new_test_data.append({'text': entry['prompt'] + ' ' + entry['chosen'], 'label': 0})\n",
    "    new_new_test_data.append({'text': entry['prompt'] + ' ' + entry['rejected'], 'label': entry['fallacy_type']})\n",
    "\n",
    "new_train_data = []\n",
    "for entry in train:\n",
    "    new_train_data.append({'text': entry['prompt'] + ' ' + entry['chosen'], 'label': 0})\n",
    "    new_train_data.append({'text': entry['prompt'] + ' ' + entry['rejected'], 'label': entry['fallacy_type']})\n",
    "\n",
    "train_data = pd.DataFrame(new_new_test_data)\n",
    "test_data = pd.DataFrame(new_train_data)\n",
    "dev_data = test_data\n",
    "\n",
    "train_input_ids, train_attention_mask, train_labels_tensor = model.data_tensor(train_data)\n",
    "dev_input_ids, dev_attention_mask, dev_labels_tensor = model.data_tensor(dev_data)\n",
    "test_input_ids, test_attention_mask, test_labels_tensor = model.data_tensor(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    logging_strategy='steps',\n",
    "    save_total_limit=1,\n",
    ")\n",
    "from collections import Counter\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred.predictions, pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    a = []\n",
    "    b = []\n",
    "    misclassified = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "       if predictions[i] != labels[i]:\n",
    "           misclassified.append(predictions[i])\n",
    "\n",
    "    print(Counter(misclassified))\n",
    " \n",
    "    print('----'*5)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    precision = precision_score(labels, predictions, average='macro')\n",
    "    recall = recall_score(labels, predictions, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model.get_model(),\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=lambda data: {\n",
    "            'input_ids': torch.stack([item[0] for item in data]),\n",
    "            'attention_mask': torch.stack([item[1] for item in data]),\n",
    "            'labels': torch.stack([item[2] for item in data])\n",
    "        },\n",
    "        train_dataset=torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_labels_tensor),\n",
    "        eval_dataset=torch.utils.data.TensorDataset(dev_input_ids, dev_attention_mask, dev_labels_tensor),\n",
    "        )    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/root/Logical-Fallacies/models/fallacy_clf_trainedontest')\n",
    "model.tokenizer.save_pretrained('/root/Logical-Fallacies/models/fallacy_clf_trainedontest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modell=AutoModelForSequenceClassification.from_pretrained('/root/Logical-Fallacies/models/fallacy_clf_withoutprompt')\n",
    "# tokenizer=AutoTokenizer.from_pretrained('/root/Logical-Fallacies/models/fallacy_clf_withoutprompt')\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = modell(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs.detach().numpy()\n",
    "\n",
    "def predict_fallacy(prompt, argument):\n",
    "    if prompt: \n",
    "        input_text = prompt + \" \" + argument\n",
    "    else:\n",
    "        input_text = argument\n",
    "    probs = predict(input_text)\n",
    "\n",
    "    predicted_label = np.argmax(probs)\n",
    "    # for key, value in CLASSES.items():\n",
    "    #     if value == predicted_label:\n",
    "    #         predicted_label = key\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/dpo_with_fallacies_test.json', 'r') as f:\n",
    "        test = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/Logical-Fallacies/results/llama/args.json', 'r') as f:\n",
    "        args = json.load(f)\n",
    "\n",
    "args = [x[1] for x in args]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosen_accuracy(arr):\n",
    "     return sum([1 for i in arr if i == 0])/len(arr)\n",
    "def rejected_accuracy(arr):\n",
    "     pred = [i[0] for i in arr]\n",
    "     true = [i[1] for i in arr]\n",
    "     return sum([1 for i in range(len(pred)) if pred[i] == true[i]])/len(pred)\n",
    "preds_chosen = []\n",
    "preds_rejected = []\n",
    "for i, sample in tqdm(enumerate(args), total=len(args)):\n",
    "     if i%50 == 0 and i >0:\n",
    "          print(\"preds chosen\")\n",
    "          print(chosen_accuracy(preds_chosen))\n",
    "          print(\"preds rejected\")\n",
    "          print(rejected_accuracy(preds_rejected))\n",
    "          print(\"----\"*10)\n",
    "     preds_chosen.append(predict_fallacy(sample['prompt'], sample['chosen']))\n",
    "     \n",
    "     preds_rejected.append((predict_fallacy(sample['prompt'], sample['rejected']), sample['fallacy_type']))\n",
    "     #print(predict_fallacy(sample['prompt'], sample['rejected']))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/train.json', 'r') as f:\n",
    "    df_train = json.load(f)\n",
    "\n",
    "with open('/root/Logical-Fallacies/data/dpo/arguments/dpo_with_fallacies_test.json', 'r') as f:\n",
    "    df_test = json.load(f)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "limit = 430 \n",
    "def downsample_data(data, split='train'):\n",
    "    training_data = []\n",
    "    chosen_samples = set()\n",
    "    num_chosen = 0\n",
    "    for i, entry in data.iterrows():\n",
    "        chosen = entry.chosen\n",
    "        num_chosen += 1\n",
    "        rejected = entry.rejected\n",
    "    \n",
    "        \n",
    "        if chosen not in chosen_samples:\n",
    "            chosen_samples.add(chosen)\n",
    "            if num_chosen <= 2000 and split=='train':\n",
    "                training_data.append({'text': chosen, 'label': INVERSE[0]})\n",
    "            elif split == 'test' and num_chosen <= 500:\n",
    "                training_data.append({'text': chosen, 'label': INVERSE[0]})\n",
    "\n",
    "\n",
    "        training_data.append({'text': rejected, 'label': INVERSE[entry['fallacy_type']]})\n",
    "    return (pd.DataFrame(training_data))\n",
    "\n",
    "CLASSES = {'Not a Fallacy': 0,  'faulty generalization': 1, 'false causality': 2, 'fallacy of relevance': 3, 'fallacy of extension': 4, 'equivocation': 5, 'ad populum': 6, 'appeal to emotion': 7, 'ad hominem': 8, 'circular reasoning': 9, 'fallacy of credibility': 10, 'fallacy of logic': 11, 'false dilemma': 12, 'intentional': 13}\n",
    "INVERSE = {v:k for k,v in CLASSES.items()}\n",
    "\n",
    "\n",
    "# for entry in df_train:\n",
    "#     train_data.append({'text': entry['chosen'], 'label': INVERSE[0]})\n",
    "#     train_data.append({'text': entry['rejected'], 'label': INVERSE[entry['fallacy_type']]})\n",
    "\n",
    "# for entry in df_test:\n",
    "#     test_data.append({'text': entry['chosen'], 'label': INVERSE[0]})\n",
    "#     test_data.append({'text': entry['rejected'], 'label': INVERSE[entry['fallacy_type']]})\n",
    "import pandas as pd\n",
    "train_data=downsample_data(pd.DataFrame(df_train), 'train').sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_data=downsample_data(pd.DataFrame(df_test), 'test').sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#Â need to encode the labels\n",
    "\n",
    "train_data.label = train_data.label.apply(lambda x: CLASSES[x])\n",
    "test_data.label = test_data.label.apply(lambda x: CLASSES[x])\n",
    "train_data\n",
    "df = train_data\n",
    "downsampled_dfs = []\n",
    "for label in df['label'].unique():\n",
    "    # Filter the dataframe by label\n",
    "    label_df = df[df['label'] == label]\n",
    "    # Sample the dataframe without replacement\n",
    "    downsampled_df = label_df.sample(min(len(label_df), 500), random_state=42)\n",
    "    downsampled_dfs.append(downsampled_df)\n",
    "\n",
    "# Concatenate all the downsampled dataframes\n",
    "balanced_df = pd.concat(downsampled_dfs, ignore_index=True)\n",
    "\n",
    "# Check the new value counts to confirm balancing\n",
    "train_data = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a46abc95b2f4d72adc6c7e34a003aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb62c373ac6042558eb2b43f6ccefcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='824' max='824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [824/824 14:02, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Misclassified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.625600</td>\n",
       "      <td>2.574665</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>Counter({'intentional': 869, 'equivocation': 230, 'faulty generalization': 91, 'fallacy of relevance': 68, 'appeal to emotion': 3})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.444100</td>\n",
       "      <td>2.328844</td>\n",
       "      <td>0.267354</td>\n",
       "      <td>Counter({'intentional': 535, 'fallacy of relevance': 284, 'faulty generalization': 93, 'appeal to emotion': 84, 'fallacy of extension': 26, 'false causality': 24, 'equivocation': 12, 'fallacy of credibility': 4, 'ad hominem': 4})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.271900</td>\n",
       "      <td>2.100634</td>\n",
       "      <td>0.474914</td>\n",
       "      <td>Counter({'intentional': 258, 'fallacy of relevance': 166, 'appeal to emotion': 92, 'false dilemma': 76, 'false causality': 54, 'circular reasoning': 37, 'faulty generalization': 33, 'fallacy of extension': 24, 'fallacy of credibility': 13, 'ad hominem': 5, 'ad populum': 4, 'fallacy of logic': 1, 'equivocation': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>1.846657</td>\n",
       "      <td>0.645361</td>\n",
       "      <td>Counter({'circular reasoning': 96, 'false causality': 91, 'intentional': 91, 'false dilemma': 72, 'fallacy of relevance': 59, 'appeal to emotion': 31, 'fallacy of extension': 25, 'ad populum': 23, 'fallacy of credibility': 8, 'fallacy of logic': 8, 'faulty generalization': 8, 'Not a Fallacy': 3, 'ad hominem': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.824900</td>\n",
       "      <td>1.617299</td>\n",
       "      <td>0.696907</td>\n",
       "      <td>Counter({'false causality': 124, 'circular reasoning': 118, 'false dilemma': 34, 'appeal to emotion': 33, 'intentional': 31, 'ad populum': 25, 'fallacy of logic': 23, 'fallacy of extension': 18, 'fallacy of relevance': 18, 'Not a Fallacy': 6, 'fallacy of credibility': 4, 'faulty generalization': 4, 'ad hominem': 3})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.677900</td>\n",
       "      <td>1.445766</td>\n",
       "      <td>0.736082</td>\n",
       "      <td>Counter({'circular reasoning': 125, 'false causality': 96, 'fallacy of logic': 41, 'ad populum': 28, 'intentional': 23, 'appeal to emotion': 21, 'fallacy of extension': 16, 'false dilemma': 11, 'Not a Fallacy': 9, 'fallacy of credibility': 7, 'ad hominem': 3, 'faulty generalization': 2, 'fallacy of relevance': 2})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.511800</td>\n",
       "      <td>1.301854</td>\n",
       "      <td>0.743643</td>\n",
       "      <td>Counter({'circular reasoning': 115, 'false causality': 94, 'fallacy of logic': 57, 'appeal to emotion': 32, 'ad populum': 20, 'intentional': 17, 'fallacy of extension': 15, 'Not a Fallacy': 10, 'fallacy of credibility': 6, 'fallacy of relevance': 3, 'ad hominem': 2, 'false dilemma': 1, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.383700</td>\n",
       "      <td>1.188743</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>Counter({'circular reasoning': 122, 'false causality': 76, 'fallacy of logic': 76, 'appeal to emotion': 20, 'fallacy of extension': 15, 'intentional': 14, 'Not a Fallacy': 13, 'ad populum': 10, 'fallacy of credibility': 9, 'fallacy of relevance': 2, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.313600</td>\n",
       "      <td>1.099768</td>\n",
       "      <td>0.767010</td>\n",
       "      <td>Counter({'circular reasoning': 103, 'false causality': 73, 'fallacy of logic': 71, 'Not a Fallacy': 22, 'ad populum': 15, 'fallacy of extension': 15, 'appeal to emotion': 14, 'intentional': 13, 'fallacy of credibility': 8, 'fallacy of relevance': 3, 'ad hominem': 1, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.189500</td>\n",
       "      <td>1.024359</td>\n",
       "      <td>0.770447</td>\n",
       "      <td>Counter({'circular reasoning': 101, 'false causality': 70, 'fallacy of logic': 65, 'intentional': 22, 'Not a Fallacy': 21, 'fallacy of extension': 16, 'appeal to emotion': 15, 'ad populum': 10, 'fallacy of credibility': 9, 'fallacy of relevance': 2, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.125100</td>\n",
       "      <td>0.967509</td>\n",
       "      <td>0.776632</td>\n",
       "      <td>Counter({'circular reasoning': 94, 'false causality': 76, 'fallacy of logic': 53, 'Not a Fallacy': 24, 'intentional': 23, 'fallacy of extension': 15, 'ad populum': 13, 'appeal to emotion': 13, 'fallacy of credibility': 6, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.099700</td>\n",
       "      <td>0.930295</td>\n",
       "      <td>0.780756</td>\n",
       "      <td>Counter({'circular reasoning': 91, 'false causality': 63, 'fallacy of logic': 63, 'intentional': 24, 'Not a Fallacy': 22, 'fallacy of extension': 15, 'appeal to emotion': 13, 'ad populum': 11, 'fallacy of credibility': 9, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.976300</td>\n",
       "      <td>0.902207</td>\n",
       "      <td>0.781443</td>\n",
       "      <td>Counter({'circular reasoning': 82, 'fallacy of logic': 80, 'false causality': 57, 'intentional': 26, 'Not a Fallacy': 21, 'fallacy of extension': 15, 'appeal to emotion': 12, 'ad populum': 11, 'fallacy of credibility': 6, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.016900</td>\n",
       "      <td>0.879155</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>Counter({'circular reasoning': 80, 'fallacy of logic': 74, 'false causality': 60, 'intentional': 26, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 10, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 6, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.865770</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>Counter({'circular reasoning': 81, 'fallacy of logic': 71, 'false causality': 59, 'intentional': 27, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 9, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 7, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.859684</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>Counter({'circular reasoning': 81, 'fallacy of logic': 71, 'false causality': 58, 'intentional': 28, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 10, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 6, 'ad hominem': 2, 'faulty generalization': 1})</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"Counter({'intentional': 869, 'equivocation': 230, 'faulty generalization': 91, 'fallacy of relevance': 68, 'appeal to emotion': 3})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'intentional': 535, 'fallacy of relevance': 284, 'faulty generalization': 93, 'appeal to emotion': 84, 'fallacy of extension': 26, 'false causality': 24, 'equivocation': 12, 'fallacy of credibility': 4, 'ad hominem': 4})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'intentional': 258, 'fallacy of relevance': 166, 'appeal to emotion': 92, 'false dilemma': 76, 'false causality': 54, 'circular reasoning': 37, 'faulty generalization': 33, 'fallacy of extension': 24, 'fallacy of credibility': 13, 'ad hominem': 5, 'ad populum': 4, 'fallacy of logic': 1, 'equivocation': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 96, 'false causality': 91, 'intentional': 91, 'false dilemma': 72, 'fallacy of relevance': 59, 'appeal to emotion': 31, 'fallacy of extension': 25, 'ad populum': 23, 'fallacy of credibility': 8, 'fallacy of logic': 8, 'faulty generalization': 8, 'Not a Fallacy': 3, 'ad hominem': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'false causality': 124, 'circular reasoning': 118, 'false dilemma': 34, 'appeal to emotion': 33, 'intentional': 31, 'ad populum': 25, 'fallacy of logic': 23, 'fallacy of extension': 18, 'fallacy of relevance': 18, 'Not a Fallacy': 6, 'fallacy of credibility': 4, 'faulty generalization': 4, 'ad hominem': 3})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 125, 'false causality': 96, 'fallacy of logic': 41, 'ad populum': 28, 'intentional': 23, 'appeal to emotion': 21, 'fallacy of extension': 16, 'false dilemma': 11, 'Not a Fallacy': 9, 'fallacy of credibility': 7, 'ad hominem': 3, 'faulty generalization': 2, 'fallacy of relevance': 2})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 115, 'false causality': 94, 'fallacy of logic': 57, 'appeal to emotion': 32, 'ad populum': 20, 'intentional': 17, 'fallacy of extension': 15, 'Not a Fallacy': 10, 'fallacy of credibility': 6, 'fallacy of relevance': 3, 'ad hominem': 2, 'false dilemma': 1, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 122, 'false causality': 76, 'fallacy of logic': 76, 'appeal to emotion': 20, 'fallacy of extension': 15, 'intentional': 14, 'Not a Fallacy': 13, 'ad populum': 10, 'fallacy of credibility': 9, 'fallacy of relevance': 2, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 103, 'false causality': 73, 'fallacy of logic': 71, 'Not a Fallacy': 22, 'ad populum': 15, 'fallacy of extension': 15, 'appeal to emotion': 14, 'intentional': 13, 'fallacy of credibility': 8, 'fallacy of relevance': 3, 'ad hominem': 1, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 101, 'false causality': 70, 'fallacy of logic': 65, 'intentional': 22, 'Not a Fallacy': 21, 'fallacy of extension': 16, 'appeal to emotion': 15, 'ad populum': 10, 'fallacy of credibility': 9, 'fallacy of relevance': 2, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 94, 'false causality': 76, 'fallacy of logic': 53, 'Not a Fallacy': 24, 'intentional': 23, 'fallacy of extension': 15, 'ad populum': 13, 'appeal to emotion': 13, 'fallacy of credibility': 6, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 91, 'false causality': 63, 'fallacy of logic': 63, 'intentional': 24, 'Not a Fallacy': 22, 'fallacy of extension': 15, 'appeal to emotion': 13, 'ad populum': 11, 'fallacy of credibility': 9, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 82, 'fallacy of logic': 80, 'false causality': 57, 'intentional': 26, 'Not a Fallacy': 21, 'fallacy of extension': 15, 'appeal to emotion': 12, 'ad populum': 11, 'fallacy of credibility': 6, 'fallacy of relevance': 5, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 80, 'fallacy of logic': 74, 'false causality': 60, 'intentional': 26, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 10, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 6, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 81, 'fallacy of logic': 71, 'false causality': 59, 'intentional': 27, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 9, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 7, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"Counter({'circular reasoning': 81, 'fallacy of logic': 71, 'false causality': 58, 'intentional': 28, 'Not a Fallacy': 25, 'fallacy of extension': 15, 'ad populum': 10, 'appeal to emotion': 9, 'fallacy of relevance': 7, 'fallacy of credibility': 6, 'ad hominem': 2, 'faulty generalization': 1})\" of type <class 'collections.Counter'> for key \"eval/misclassified\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=824, training_loss=1.5498591895821026, metrics={'train_runtime': 844.2856, 'train_samples_per_second': 62.074, 'train_steps_per_second': 0.976, 'total_flos': 6105329894627328.0, 'train_loss': 1.5498591895821026, 'epoch': 8.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# train_data = pd.DataFrame(train_data)\n",
    "# train_data = train_data.sample(frac=1).reset_index(drop=True)[:1000]\n",
    "# test_data = pd.DataFrame(test_data)\n",
    "# test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "# Load the tokenizer and model\n",
    "model_name = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=14, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Assume `data` is your dataset loaded into a DataFrame with columns \"text\" and \"label\"\n",
    "# Replace this with your actual data loading code\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "\n",
    "# Create a Dataset object\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "# train_test_dataset = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "# if train_test_dataset['train'].num_rows == 0 or train_test_dataset['test'].num_rows == 0:\n",
    "#     raise ValueError(\"One of the splits ended up being empty. Adjust your train-test split.\")\n",
    "\n",
    "train_test_dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # output directory\n",
    "    num_train_epochs=8,               # number of training epochs\n",
    "    per_device_train_batch_size=8,    # batch size for training\n",
    "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
    "    warmup_steps=0,                 # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.1,                # strength of weight decay\n",
    "    logging_dir='./logs',             # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    learning_rate=2e-6,\n",
    "    save_steps=3000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\"\n",
    ")\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred.predictions, pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    \n",
    "    misclassified = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "       if predictions[i] != labels[i]:\n",
    "           misclassified.append(INVERSE[predictions[i]])\n",
    "\n",
    "    return {\n",
    "        'accuracy': (predictions == labels).mean(),\n",
    "        'misclassified': Counter(misclassified)\n",
    "    }\n",
    " \n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test_dataset['train'],\n",
    "    eval_dataset=train_test_dataset['test'],\n",
    "    compute_metrics=compute_metrics # Implement more sophisticated metrics as needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/Logical-Fallacies/src/classifier/clf2.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m/root/Logical-Fallacies/models/fallacy/clf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m/root/Logical-Fallacies/models/fallacy/clf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model('/root/Logical-Fallacies/models/fallacy/clf')\n",
    "tokenizer.save_pretrained('/root/Logical-Fallacies/models/fallacy/clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def predict_fallacy(prompt, argument):\n",
    "    if prompt: \n",
    "        input_text = prompt + \" \" + argument\n",
    "    else:\n",
    "        input_text = argument\n",
    "    probs = predict(input_text)\n",
    "\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    # for key, value in CLASSES.items():\n",
    "    #     if value == predicted_label:\n",
    "    #         predicted_label = key\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not a Fallacy': 0,\n",
       " 'ad hominem': 1,\n",
       " 'ad populum': 2,\n",
       " 'appeal to emotion': 3,\n",
       " 'circular reasoning': 4,\n",
       " 'equivocation': 5,\n",
       " 'fallacy of relevance': 6,\n",
       " 'false causality': 7,\n",
       " 'false dilemma': 8,\n",
       " 'faulty generalization': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keys = ['Not a Fallacy', 'ad hominem', 'ad populum', 'appeal to emotion',\n",
    " 'circular reasoning', 'equivocation', 'fallacy of relevance',\n",
    " 'false causality' ,'false dilemma' ,'faulty generalization']\n",
    "\n",
    "d = {k: i for i, k in enumerate(keys)}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/Logical-Fallacies/src/classifier/clf2.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m chosen \u001b[39m=\u001b[39m entry[\u001b[39m'\u001b[39m\u001b[39mchosen\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m rejected \u001b[39m=\u001b[39m entry[\u001b[39m'\u001b[39m\u001b[39mrejected\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m pred \u001b[39m=\u001b[39m predict_fallacy(instruction, chosen)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m pred \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     accuracy_chosen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/root/Logical-Fallacies/src/classifier/clf2.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     input_text \u001b[39m=\u001b[39m argument\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m probs \u001b[39m=\u001b[39m predict(input_text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(probs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# for key, value in CLASSES.items():\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#     if value == predicted_label:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#         predicted_label = key\u001b[39;00m\n",
      "\u001b[1;32m/root/Logical-Fallacies/src/classifier/clf2.ipynb Cell 36\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(text):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster131.iccluster.epfl.ch/root/Logical-Fallacies/src/classifier/clf2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m probs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1539\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1539\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1540\u001b[0m     input_ids,\n\u001b[1;32m   1541\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1542\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1543\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1544\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1545\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1546\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1547\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1548\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1549\u001b[0m )\n\u001b[1;32m   1551\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1553\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:981\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 981\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    982\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    983\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    984\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    985\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    986\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    987\u001b[0m )\n\u001b[1;32m    988\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    989\u001b[0m     embedding_output,\n\u001b[1;32m    990\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:207\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    204\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    208\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    210\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "chosen = df_test['chosen'].unique()\n",
    "\n",
    "rejected = df_test['rejected'].unique()\n",
    "accuracy_chosen = 0 \n",
    "accuracy_rejected = 0\n",
    "from tqdm import tqdm\n",
    "for i, entry in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    instruction = entry['prompt']\n",
    "    chosen = entry['chosen']\n",
    "    rejected = entry['rejected']\n",
    "    pred = predict_fallacy(instruction, chosen)\n",
    "    if pred == 0:\n",
    "        accuracy_chosen += 1\n",
    "    \n",
    "    pred_rejected = predict_fallacy(instruction, rejected)\n",
    " \n",
    "\n",
    "    if le.inverse_transform([pred_rejected.detach().item()])[0] == INVERSE[entry['fallacy_type']]:\n",
    "        accuracy_rejected += 1\n",
    "\n",
    "\n",
    "print(accuracy_chosen)\n",
    "print(accuracy_rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 400/400 [00:12<00:00, 32.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/root/Logical-Fallacies/models/fallacy/clf').to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/root/Logical-Fallacies/models/fallacy/clf')\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def predict_fallacy(prompt, argument):\n",
    "    if prompt: \n",
    "        input_text = prompt + \" \" + argument\n",
    "    else:\n",
    "        input_text = argument\n",
    "    probs = predict(input_text)\n",
    "    \n",
    "    predicted_label = torch.argmax(probs)\n",
    "\n",
    "    # for key, value in CLASSES.items():\n",
    "    #     if value == predicted_label:\n",
    "    #         predicted_label = key\n",
    "    return predicted_label\n",
    "keys = ['Not a Fallacy', 'ad hominem', 'ad populum', 'appeal to emotion',\n",
    " 'circular reasoning', 'equivocation', 'fallacy of relevance',\n",
    " 'false causality' ,'false dilemma' ,'faulty generalization']\n",
    "\n",
    "d = {k: i for i, k in enumerate(keys)}\n",
    "d\n",
    "\n",
    "import json \n",
    "with open('/root/Logical-Fallacies/results/llama/arguments/sft/f-rate.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "acc= 0\n",
    "misclassified = []\n",
    "from tqdm import tqdm\n",
    "for entry in tqdm(data):\n",
    "    f_type = entry['fallacy_type']\n",
    "    if f_type=='None':\n",
    "        f_type = 'Not a Fallacy'\n",
    "\n",
    "    argument = entry['argument']\n",
    "    topic = entry['topic']\n",
    "    pred = predict_fallacy(None, argument)\n",
    "    pred= INVERSE[pred.detach().item()]\n",
    "\n",
    "    if pred != f_type:\n",
    "        misclassified.append(pred)\n",
    "\n",
    "    if pred == f_type: \n",
    "       acc +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Not a Fallacy': 86,\n",
       "         'ad populum': 1,\n",
       "         'false causality': 12,\n",
       "         'circular reasoning': 60,\n",
       "         'intentional': 13,\n",
       "         'fallacy of logic': 10,\n",
       "         'ad hominem': 5,\n",
       "         'fallacy of relevance': 11,\n",
       "         'fallacy of credibility': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appeal to emotion']\n",
      "['Not a Fallacy' 'ad hominem' 'ad populum' 'appeal to emotion'\n",
      " 'circular reasoning' 'equivocation' 'fallacy of relevance'\n",
      " 'false causality' 'false dilemma' 'faulty generalization']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If people feel strongly enough to protest loudly, their opinion should be taken into consideration.\n",
      "tensor(7, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:01, 14.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If journalism was subsidized, Newspapers could keep their level of readership up in the digital age.\n",
      "tensor(7, device='cuda:0')\n",
      "If journalism was subsidized, Newspapers could keep their level of readership up in the digital age.\n",
      "tensor(7, device='cuda:0')\n",
      "If journalism was subsidized, Newspapers could keep their level of readership up in the digital age.\n",
      "tensor(7, device='cuda:0')\n",
      "If journalism was subsidized, Newspapers could keep their level of readership up in the digital age.\n",
      "tensor(7, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(le.inverse_transform([3]))\n",
    "print(le.classes_)\n",
    "num_miss = 0\n",
    "for i, entry in tqdm(enumerate(df_test)):\n",
    "    chosen = entry['chosen']\n",
    "    rejected = entry['rejected']\n",
    "    chosen_pred = predict_fallacy(None, chosen)\n",
    "    rejected_pred = predict_fallacy(None, rejected)\n",
    "\n",
    "    if chosen_pred != 0:\n",
    "        print(chosen)\n",
    "        num_miss+=1\n",
    "        print(chosen_pred)\n",
    "    # if le.inverse_transform([rejected_pred.detach().cpu().item()])[0] !=INVERSE[entry['fallacy_type']]:\n",
    "    #     print(rejected)\n",
    "    #     print(INVERSE[entry['fallacy_type']])\n",
    "    #     print(le.inverse_transform([rejected_pred.detach().cpu().item()])[0])\n",
    "    #     num_miss+=1\n",
    "\n",
    "    if num_miss == 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
